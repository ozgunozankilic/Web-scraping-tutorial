{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping basics with Selenium\n",
    "\n",
    "Unofficial Selenium documentation: https://selenium-python.readthedocs.io/\n",
    "\n",
    "Versions used in this notebook:\n",
    "* Python 3.10.2\n",
    "* Selenium 4.1.3\n",
    "* ChromeDriver 99.0.4844.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WebDriver\n",
    "\n",
    "1. Download the driver.\n",
    "    * [ChromeDriver](https://sites.google.com/chromium.org/driver/) for Google Chrome\n",
    "    * [GeckoDriver](https://github.com/mozilla/geckodriver/releases) for Mozilla Firefox\n",
    "1. Put it in somewhere Selenium can find it.\n",
    "    * Option 1: In the same folder with the script\n",
    "    * Option 2: In a folder that is included in the system path (for Windows)\n",
    "    * Option 3: Add its location to the system path (for Windows)\n",
    "\n",
    "### Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some options\n",
    "\n",
    "It is possible to define the browser position/size, user agent, proxy, etc. Note that the option object is browser-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Creates an options object.\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Changes the user agent.\n",
    "user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n",
    "options.add_argument(\"--user-agent=%s\" % user_agent)\n",
    "\n",
    "# Proxy?\n",
    "# proxy = \"<IP>:<PORT>\"\n",
    "# options.add_argument(\"--proxy-server=%s\" % proxy)\n",
    "\n",
    "# Removes certain fields that can be used to detect WebDriver.\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Sets English as the only accepted language.\n",
    "options.add_experimental_option(\"prefs\", {\"intl.accept_languages\": \"en;q=0.9\"})\n",
    "\n",
    "# Opens the browser in the incognito mode.\n",
    "options.add_argument(\"--incognito\")\n",
    "\n",
    "# Makes the browser headless.\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "# Opens the browser window with the provided options.\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Sets window size.\n",
    "driver.set_window_size(1400, 900)\n",
    "\n",
    "# Sets window position.\n",
    "driver.set_window_position(500, 0)\n",
    "\n",
    "# Sets timeout threshold.\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page navigation\n",
    "\n",
    "### Current tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://atomurl.net/myip/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New tab\n",
    "\n",
    "While Firefox allows sending key combinations such as Ctrl + T, Chrome requires executing JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tab_url = \"https://www.hepsiemlak.com/ankara-satilik\"\n",
    "driver.execute_script(f\"window.open('{new_tab_url}','_blank')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab switching\n",
    "\n",
    "We can locate a tab using indices and switch to that tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first tab has an index of 0.\n",
    "first_tab = driver.window_handles[0]\n",
    "\n",
    "driver.switch_to.window(first_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing a specific tab\n",
    "\n",
    "Closing a tab forces switching to the last open tab, but we need to explicitly change focus to a certain open tab. Otherwise, bugs can occur in the future. To make things simpler, we can write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 means the last index is used. Even if -1 is used to close a tab, once the tab is closed, -1 refers to the one comes just before the closed one.\n",
    "def close_tab(index_to_close, index_to_switch_after=-1):\n",
    "    driver.switch_to.window(driver.window_handles[index_to_close])\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[index_to_switch_after])\n",
    "\n",
    "\n",
    "# close_tab(-1, -1) # Close the last tab, switch to the last remaining tab\n",
    "# close_tab(0, 0) # Close the first tab, switch to the first remaining tab\n",
    "close_tab(0, -1)  # Close the first tab, switch to the last remaining tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating elements\n",
    "\n",
    "All possible ways: https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "Some common ones:\n",
    "* find_element_by_id\n",
    "* find_element_by_class_name\n",
    "* find_elements_by_class_name\n",
    "* find_element_by_tag_name\n",
    "* find_elements_by_tag_name\n",
    "* find_element_by_xpath\n",
    "* find_elements_by_xpath\n",
    "\n",
    "Using the \"find_element\" versions retrieve only the first occurrences.\n",
    "\n",
    "Xpath cheat sheet: https://devhints.io/xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the number of results\n",
    "\n",
    "We firstly obtain the text inside the element and then process the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24781"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_text = driver.find_element_by_class_name(\"applied-filters__count\").text\n",
    "\n",
    "result_int = int(result_text.split(\"için \")[1].split(\" ilan\")[0].replace(\".\", \"\"))\n",
    "\n",
    "result_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving an element using ID and retrieving the entire HTML content in it\n",
    "\n",
    "This time we get everything including the text and HTML tags inside the element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<a href=\"/ankara-satilik\" title=\"Ankara Satılık Ev\" data-v-05396ec5=\"\">\\n        Tüm Sonuçlar\\n      </a>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tab = driver.find_element_by_id(\"realty-owner-all-results\")\n",
    "\n",
    "# results_tab.text # This would simply retrieve the text without any tags\n",
    "results_tab.get_attribute(\"innerHTML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving all links using their tag (\"a\") and counting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = driver.find_elements_by_tag_name(\"a\")\n",
    "\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving all links that come right inside `<li>` and have an actual target instead of \"#\"\n",
    "\n",
    "We will simply print the first 10 targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.hepsiemlak.com/ankara-satilik\n",
      "https://www.hepsiemlak.com/ankara-kiralik\n",
      "https://www.hepsiemlak.com/ankara-gunluk-kiralik\n",
      "https://www.hepsiemlak.com/proje/ankara-projeleri\n",
      "https://www.hepsiemlak.com/istanbul-satilik\n",
      "https://www.hepsiemlak.com/ankara-satilik\n",
      "https://www.hepsiemlak.com/izmir-satilik\n",
      "https://www.hepsiemlak.com/adana-satilik\n",
      "https://www.hepsiemlak.com/adiyaman-satilik\n",
      "https://www.hepsiemlak.com/afyonkarahisar-satilik\n"
     ]
    }
   ],
   "source": [
    "meaningful_list_links = driver.find_elements_by_xpath('//li/a[@href!=\"#\"]')\n",
    "\n",
    "for link in meaningful_list_links[:10]:\n",
    "    print(link.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction\n",
    "\n",
    "Let us apply a filter to these results.\n",
    "\n",
    "### Clicking on an element\n",
    "\n",
    "We will firstly choose a district. To do so, we will click on the district selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_selector = driver.find_element_by_xpath(\n",
    "    '//section[@class=\"filter-item-wrap loc locationCountySec\"]'\n",
    ")\n",
    "\n",
    "district_selector.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing in\n",
    "\n",
    "We can now start typing in the input field that shows up. To do so, we will locate the input field inside the selector and send our district of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we use the previously obtained element to directly search inside the element\n",
    "district_search = district_selector.find_element_by_xpath(\n",
    "    './/input[@class=\"he-select-base__search\"]'\n",
    ")\n",
    "\n",
    "district = \"Çankaya\"\n",
    "\n",
    "district_search.send_keys(district)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have a visible district element that we can click on. This element is inside a div that follows the input field and has a class of \"ps\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_search.find_element_by_xpath(\n",
    "    './following-sibling::div[@class=\"ps\"]//a[@class=\"js-county-filter__list-link\"]'\n",
    ").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this filter by clicking on the search button:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two buttons in this section, but we need only the first one.\n",
    "driver.find_element_by_xpath('//section[@class=\"filter-button-wrapper\"]/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "Now that we know how to navigate inside the page and between tabs, we can start collecting information. Let us firstly find how many results we now have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7066"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_text = driver.find_element_by_class_name(\"applied-filters__count\").text\n",
    "result_int = int(result_text.split(\"için \")[1].split(\" ilan\")[0].replace(\".\", \"\"))\n",
    "result_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve each real estate ad (except the ones that are external ads leading to other domains) on the page and count them. This part can be tricky and messy in certain websites especially since what we are trying to retrieve can be represented in different forms, which requires careful HTML inspection and some experimentation. It is also possible to find alternative approaches here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_cards = driver.find_elements_by_xpath(\n",
    "    '//div[@class=\"listView\"]/div[contains(@class, \"listing-project\") or @data-v-7e43f948][@class!=\"w100p realty-owner-tab\"]'\n",
    ")\n",
    "\n",
    "len(ad_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the pagination section. This part is crucial when the content is divided into pages. We see that page links are actually given in an unordered list (`<ul data-v-4ff47588=\"\" class=\"he-pagination__links\">`). While we cannot see the links of all pages, we can see that the last list item always corresponds to the last page. Therefore, we can obtain the last list item's text here and find the total number of pages. Instead of retrieving all elements and obtaining the last one, we can also retrieve the link for only the last list item. Then, we can get the link text and obtain an integer from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use \"starts-with\" here because the active page has an extra class name in it.\n",
    "last_page = int(\n",
    "    driver.find_element_by_xpath(\n",
    "        '//li[starts-with(@class, \"he-pagination__item\")][last()]/a'\n",
    "    ).text\n",
    ")\n",
    "\n",
    "last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_int - len(ad_cards) < last_page * len(ad_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is peculiar, because we would expect to see the difference between the actual number of ads and the possible number of ads to be less than the number of ads shown on a single page, but it is not the case. After some manual examination of different pages, it seems the difference may stem from caching issue or something since each page (except the last page) indeed has 24 relevant ads.\n",
    "\n",
    "At this point, we have different options:\n",
    "* We can firstly visit all pages to retrieve the listing links and then visit each ad later.\n",
    "* We can incrementally scrape all pages, directly starting scraping the ads one by one.\n",
    "\n",
    "They have their advantages and disadvantages. We will choose the latter here.\n",
    "\n",
    "We also need to decide on how we are going to navigate between pages. Instead of finding the relevant page link and clicking on it, we can actually use a shortcut here. Notice that the page number is parameterized as a HTTP GET parameter in the URL. So, when we go to the second page, we see `?page=2` being appended to the page URL. Therefore, we can generate the URL for each page and directly visit them instead. This saves us from juggling multiple tabs as well. If it would not be possible to visit each page using URLs, we would need to keep the results page, open new tabs each time we want to visit a specific ad, collect information, close that tab, and repeat. Now, once we visit a result page, we can extract all links and visit them one by one before we move on to the next page's URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.hepsiemlak.com/cankaya-satilik?page=1', 'https://www.hepsiemlak.com/cankaya-satilik?page=2', 'https://www.hepsiemlak.com/cankaya-satilik?page=3', 'https://www.hepsiemlak.com/cankaya-satilik?page=4', 'https://www.hepsiemlak.com/cankaya-satilik?page=5']\n"
     ]
    }
   ],
   "source": [
    "current_url = driver.current_url\n",
    "\n",
    "page_links = [f\"{current_url}?page={index}\" for index in range(1, last_page+1)]\n",
    "\n",
    "# Printing only the first five links:\n",
    "print(page_links[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things short, we will loop over only the first two pages and only the first five ads on each page. For each page, we will retrieve the ads, extract their links, and accumulate them in a page-specific list. Then, we will visit each ad's page. For now, we will simply collect the sale price from them. It is possible to collect this without ever visiting a specific ad's page, but it would be too easy.\n",
    "\n",
    "Once we are done visiting all ads on a page, we will continue with the following results page. Firstly, let us write a function that deals with scraping the ad information for a given results page URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "import time\n",
    "\n",
    "def scrape_results(results_page, driver):\n",
    "    # Visits the results page\n",
    "    driver.get(results_page)\n",
    "\n",
    "    # These will be filled in\n",
    "    ad_links = []\n",
    "    sale_prices = []\n",
    "\n",
    "    # Retrieving the ads after waiting for them to be available. Otherwise, WebDriver may fail to locate them.\n",
    "    # More information on this can be found here: https://selenium-python.readthedocs.io/waits.html\n",
    "    # Note that you we could also use the sleeping function to wait a few seconds before we try to retrieve any elements.\n",
    "    wait = WebDriverWait(driver, 10) # It will wait up to 10 seconds until the necessary elements are \n",
    "    ad_cards = wait.until(EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"listView\"]/div[contains(@class, \"listing-project\") or @data-v-7e43f948][@class!=\"w100p realty-owner-tab\"]')))\n",
    "    \n",
    "    print(f\"Scraping {results_page}. There are {len(ad_cards)} ads on this page.\")\n",
    "\n",
    "    # For the first five ads, we need to find the link\n",
    "    for ad_card in ad_cards[:5]:\n",
    "        # Different types of cards have their links at different places.\n",
    "        # When we are not sure whether an element exists, we can use \"find_elements\" and then count the retrieved elements. 0 elements mean there is no match.\n",
    "        \n",
    "        links_alt1 = ad_card.find_elements_by_xpath('.//a[contains(@href, \"/proje/\")]')\n",
    "        links_alt2 = ad_card.find_elements_by_xpath('.//div[@class=\"list-view-content\"]/div[@class=\"links\"]/a')\n",
    "        \n",
    "        if len(links_alt1) > 0:\n",
    "            link = links_alt1[0].get_attribute(\"href\")\n",
    "        elif len(links_alt2) > 0:\n",
    "            link = links_alt2[0].get_attribute(\"href\")\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        ad_links.append(link)\n",
    "    \n",
    "    # For each individual ad\n",
    "    for ad_link in ad_links:\n",
    "        # Visits the ad page\n",
    "        driver.get(ad_link)\n",
    "        \n",
    "        # Waiting for a constant amount for the page to load\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # If price exists and its value is valid, it is appended to the price list\n",
    "        prices = driver.find_elements_by_xpath('//p[contains(@class, \"price\")]')\n",
    "        if len(prices) > 0:\n",
    "            # Some formatting to remove unnecessary text\n",
    "            price = prices[0].text.strip().split()[0].replace(\".\", \"\")\n",
    "            if price.isdigit():\n",
    "                price = int(price)\n",
    "                sale_prices.append(price)\n",
    "        \n",
    "        sleep_duration = random.uniform(3, 5)\n",
    "        time.sleep(sleep_duration)\n",
    "                \n",
    "    return sale_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to test such functions before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_results('https://www.hepsiemlak.com/cankaya-satilik?page=1', driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can loop over the result pages and run this function for all of them. Notice that we put the program to sleep between visiting ad pages. This is done so that the server does not get overwhelmed or does not think we are malicious. Randomizing the sleep duration helps with leaving a more human-like impression. While we do not wait much long here, it is advisable to wait at least 10 seconds between each request just to be sure.\n",
    "\n",
    "Note that every time we visit a new page, something can go wrong. An element may not exist, the data may change in the middle of the session, the website may decide to show an alert or pop-up-like message that was never shown before, etc. This would make your program throw an exception. While it is possible to write more elaborate measures, we can simply put it inside a try block, count the number of consecutively failed pages, and stop when the number reaches 3 (we would never reach it here since we only scrape the first two pages).\n",
    "\n",
    "We can combine all of the scraped prices in a single list that can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.hepsiemlak.com/cankaya-satilik?page=1. There are 24 ads on this page.\n",
      "Scraping https://www.hepsiemlak.com/cankaya-satilik?page=2. There are 24 ads on this page.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[955000,\n",
       " 1295000,\n",
       " 1350000,\n",
       " 1100000,\n",
       " 5750000,\n",
       " 965000,\n",
       " 2200000,\n",
       " 875000,\n",
       " 969000,\n",
       " 955000]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consecutive_failures = 0\n",
    "all_sale_prices = []\n",
    "\n",
    "# For the first two results pages\n",
    "for page_link in page_links[:2]:\n",
    "    try:\n",
    "        # Scrapes the current page\n",
    "        page_sale_prices = scrape_results(page_link, driver)\n",
    "        \n",
    "        # Extends the page prices to the list\n",
    "        all_sale_prices.extend(page_sale_prices)\n",
    "        \n",
    "        # Resets the failure counter\n",
    "        consecutive_failures = 0\n",
    "    except:\n",
    "        # Increments the failure counter\n",
    "        consecutive_failures += 1\n",
    "    \n",
    "    if consecutive_failures > 2:\n",
    "        break\n",
    "        \n",
    "all_sale_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrolling\n",
    "\n",
    "We can simulate scrolling using JavaScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, simply scrolling to the end of the page may not be enough. Some pages implement infinite scrolling instead of pagination. In that case, we need to scroll, wait for the new content, and repeat. Note that it can potentially lead to scrolling infinitely or getting kicked out of the website, so make necessary changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_continuously(wait_between=1):\n",
    "    # Taken from: https://stackoverflow.com/a/27760083/4825304\n",
    "    \n",
    "    # Retrieves the current scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Waits a second for the new content\n",
    "        time.sleep(wait_between)\n",
    "\n",
    "        # Retrieves the new scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # If the scroll height did not change, it stops\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        \n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://infinite-scroll.com/demo/masonry/page4.html\")\n",
    "time.sleep(2)\n",
    "\n",
    "scroll_continuously(wait_between=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "author": "Özgün Ozan Kılıç",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
